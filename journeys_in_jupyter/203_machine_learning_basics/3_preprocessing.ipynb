{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "<hr>\n",
    "<br>\n",
    "<h1 align='center'>Preprocessing</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "            <img src=\"static/log_transform.svg\" width=\"200\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: top;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">\"Give me six hours to chop down a tree and I will spend the first four sharpening the axe.\"</p>\n",
    "                <br>\n",
    "                <p>-Abraham Lincoln</p>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Population_vs_area.svg'>Skbkekas</a> under the <a href='https://creativecommons.org/licenses/by-sa/3.0/deed.en'>CC BY-SA 3.0</a>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the things.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally\n",
    "\n",
    "Good data will beat good algorithms 9 times out of 10. ML algorithms are finicky beasts, and if you do not adequately clean and prepare your data your results will be garbage. In this section, we're going to run through some of the more useful data processing operations and also take some detours into related data concepts.\n",
    "\n",
    "Outside of the pandas library, most of our preprocessing will come from the following modules:\n",
    "\n",
    "* [sklearn.preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "* [sklearn.model_selection](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [sklearn.feature_selection](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)\n",
    "* [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction)\n",
    "* [sklearn.decomposition](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)\n",
    "\n",
    "\n",
    "For additional detail on cleaning data, see the \"Automation and Cleaning Data in Python\" presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Train test split\n",
    "\n",
    "Though it's more of a model validation strategy, [train test splits](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets) are done prior to model creation which is why we will address them here.\n",
    "\n",
    "* [Train Test Split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "As mentioned in the last section, we **train** our algorithm using data prior predicting with it. We can then **test** the predictive power of our model (i.e. how good or bad it is at a particular job) by validating or analyzing its results. The data you use for training and testing should be separate--otherwise your model will be able to perfectly score what it has seen, but won't be able to predict anything useful for unseen data. This is known as **[overfitting](https://en.wikipedia.org/wiki/Overfitting)**.\n",
    "\n",
    "In other words, you take a large portion of your data, and use it to train your model. You then use the remaining data to test on. Obviously this means that you are taking away training data from your model, which can lead to inferior results. We will discuss cross-validation as a strategy to mitigate this issue in the validation notebook.\n",
    "\n",
    "For an excellent demo, see [Scikit: Underfitting vs. Overfitting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)\n",
    "\n",
    "### If you learn nothing else, learn that you should always segregate your training and testing data.\n",
    "\n",
    "Note: I generally leave the train test split ratio at .25, but [opinions differ on the optimal split](https://stats.stackexchange.com/questions/113994/how-to-choose-the-training-cross-validation-and-test-set-sizes-for-small-sampl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what's the best way to split into testing and training sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data/othello.csv', index_col=0)\n",
    "\n",
    "# How to split randomly split a dataset in one line\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['experience'], df['salary'], test_size=.2)\n",
    "\n",
    "print('Train X')\n",
    "print(X_train)\n",
    "print()\n",
    "\n",
    "print('Train y')\n",
    "print(y_train)\n",
    "print()\n",
    "\n",
    "print('Orig df')\n",
    "print(df['experience'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Whether you see it or not, machine learning algorithms work entirely in the realm of numbers. Many machine learning algorithms (i.e. all of them except for decision trees) need to be given \"scaled\" values. This is referred to as [\"feature scaling\" or \"normalization\"](https://en.wikipedia.org/wiki/Feature_scaling). If algorithms aren't given scaled features, they are likely to give you garbage because they will attribute extra importance to the larger values.\n",
    "\n",
    "We generally do this via [Z-scores](https://en.wikipedia.org/wiki/Standard_score) and outlier removal. Your workhorses here will be the StandardScaler and RobustScaler. These are largely the same, but the RobustScaler has better support for outlier smoothing.\n",
    "\n",
    "* [RobustScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)\n",
    "* [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our scalers.\n",
    "rs = RobustScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "# This needs to be fitted and transformed as it is used to convert.\n",
    "print(rs.fit_transform(df))\n",
    "\n",
    "# rs.fit_transform(df) is the same as:\n",
    "# rs.fit(df)\n",
    "# rs.transform(df)\n",
    "\n",
    "# Display nicely.\n",
    "pd.DataFrame(\n",
    "    np.concatenate([\n",
    "        df.values,\n",
    "        rs.fit_transform(df),\n",
    "        ss.fit_transform(df)\n",
    "    ], axis=1),\n",
    "    index=df.index,\n",
    "    columns=[\n",
    "        'experience',\n",
    "        'salary',\n",
    "        'robust_experience_z',\n",
    "        'robust_salary_z',\n",
    "        'standard_experience_z',\n",
    "        'standard_salary_z'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's easier to keep track of, you can also just do this inside the dataframe.\n",
    "df['salary'] = ss.fit_transform(df[['salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Imputing\n",
    "\n",
    "Did I mention that ML algorithms are tempermental yet? They also don't like missing values. Or positive infinite values. Or negative infinite values. Or anything larger than a 64-bit float, honestly. Your average ML algorithm will throw a fit if you give it a null value. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'weight': [1, np.NaN, 5, 2, 4],\n",
    "    'height': [np.NaN, 3, 1, np.NaN, 8]\n",
    "})\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "try:\n",
    "    # This is a model so it uses fit() and predict().\n",
    "    svc.fit(\n",
    "        df['weight'].values.reshape(-1,1), \n",
    "        df['height'].values.reshape(-1,1)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do we get around this? We either drop all the non-compliant values using [DataFrame.dropna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html), replace the values a specified value using [DataFrame.fillna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) or we replace or **impute** a value to the observation. Sklearn has a handy Imputer class to handle this last case.\n",
    "\n",
    "* [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer does a fit() then a transform() to convert the values.\n",
    "imp = Imputer(strategy='median', missing_values=np.NaN, axis=1)\n",
    "imp.fit_transform([[np.NaN, 1.0, 2.5, 3.0, np.NaN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.fillna(df['height'].max()) # can be arbitrary\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## One-Hot Encoding & Label Encoding\n",
    "\n",
    "Whether it's immediately apparent or not, ML algorithms use numbers and only numbers. It may look like 'setosa' or 'virginica' to you, but to the computer it's 0 or 1. While a lot of the time you can simply supply strings to machine learning algorithms and have them do all the transformational nastiness automatically, sometimes you have to get your hands dirty and [do it yourself](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features). To that end, we're going to take a look at LabelEncoder and OneHotEncoder.\n",
    "\n",
    "* [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "* [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "The label encoder takes your cateogires and transforms them into simple numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data.\n",
    "df = pd.DataFrame({\n",
    "    'type': ['puffin', 'seagull', 'puffin', 'owl', 'hermit crab'],\n",
    "    'weight': [4, 1, 4, 3, 2]\n",
    "})\n",
    "\n",
    "# Create our encoders.\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are putting in values and changing them, so we transform\n",
    "results = le.fit_transform(df['type'].values)\n",
    "\n",
    "print('Our number-ified categories are: ', end='')\n",
    "print(results)\n",
    "print('Which corresponds with:', end='')\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "print(le.inverse_transform(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You could feed this directly to your ML algorithm, BUT it would be would view an owl (1) as closer to a hermit crab (0) than to a seagull (3). Viewing these as numbers can be problematic, so it makes more sense to turn them into separate binary dimensions like \"is a seagull\", \"is a crab\", \"is an owl\". Also, note that LabelEncoder is ordered based on string name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit and transform because this is a converter.\n",
    "one_hot_data = ohe.fit_transform(results.reshape(-1,1))\n",
    "\n",
    "# Put in a dataframe and make boolean\n",
    "binary_df = pd.DataFrame(\n",
    "    one_hot_data,\n",
    "    columns=le.classes_\n",
    ").astype(bool)\n",
    "\n",
    "# Put it all together.\n",
    "pd.concat([df, binary_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's less performant, but you can also use pandas directly:\n",
    "pd.get_dummies(df['type']).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Polynomial Features\n",
    "\n",
    "Not everything can be linear, unfortunately. That said, sklearn has a [PolynomialFeatures mechanism](http://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features) which essentially maps polynomials to a matrix which can be mapped via linear regression. How does a linear regression fit a \"non-linear\" function? [Math](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html).\n",
    "\n",
    "* [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and show dataset\n",
    "df = pd.read_csv('data/othello.csv', index_col=0)\n",
    "ax = df.plot.scatter(x='experience', y='salary')\n",
    "\n",
    "# Don't worry about this. We'll get to this later.\n",
    "pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('line', LinearRegression(fit_intercept=True))\n",
    "])\n",
    "\n",
    "# This is a pipe, so we only need fit() and predict().\n",
    "pipe.fit(\n",
    "    df['experience'].values.reshape(-1,1),\n",
    "    df['salary'].values.reshape(-1,1)\n",
    ")\n",
    "\n",
    "# Linspace is awesome.\n",
    "linx = np.linspace(0,25, 1000).reshape(-1,1)\n",
    "\n",
    "ax.plot(linx, pipe.predict(linx), color='salmon', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does that work?\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Define data.\n",
    "data = np.array([\n",
    "    [0], \n",
    "    [1], \n",
    "    [2]\n",
    "])\n",
    "\n",
    "# Fit transform data which can then be mapped by complex linear regressions.\n",
    "pf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other transformations\n",
    "\n",
    "Sklearn also gives you the flexibility to put arbitrary functions in a given pipeline using a [function transformer](http://scikit-learn.org/stable/modules/preprocessing.html#function-transformer). This means that you can supply your own functions such as logs or inverse logs to arbitrarily transform arrays. Logarithmic transformations are useful for features that do not scale linearly. \n",
    "\n",
    "* [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log transformer\n",
    "function_transformer = FunctionTransformer(np.log10)\n",
    "\n",
    "# Create data\n",
    "X = np.arange(1,11,1).reshape(-1,1)\n",
    "\n",
    "# Map data.\n",
    "function_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "More features isn't necessarily better (see [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)). You generally want to eliminate less features, which can be done via statistical tests, model usage, or recursive feature selection (RFE).\n",
    "\n",
    "* [SelectFromModel](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel)\n",
    "* [RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)\n",
    "* [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n",
    "\n",
    "Lets select our \"best\" features with each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "X = df.iloc[:,[0,1,2,3]]\n",
    "y = df.iloc[:,[4]].values.ravel()\n",
    "\n",
    "# Create and fit base model for RFE and SelectFromModel\n",
    "dtree = RandomForestClassifier()\n",
    "dtree.fit(X ,y)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Via SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features by wrapping in SelectFromModel.\n",
    "model = SelectFromModel(dtree, prefit=True)\n",
    "new = model.transform(X)\n",
    "\n",
    "# Get best features out.\n",
    "best_features = X.columns[model.get_support()]\n",
    "bf_as_list = best_features.values.tolist()\n",
    "print('Decision tree says our best features are {}.'.format(bf_as_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Via RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features by wrapping in REFE \n",
    "model = RFE(estimator=dtree, n_features_to_select=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get best features out.\n",
    "ranking = model.ranking_\n",
    "rank_names = [X.columns[rank - 1] for rank in ranking]\n",
    "print('Recursive feature elimination says our best ranking is: {} if selecting only one feature.'.format(rank_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Via Kbest (requires no previous model, as it's a statistical test)\n",
    "model = SelectKBest(chi2, k=2)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get best features out\n",
    "best_features = X.columns[model.get_support()]\n",
    "bf_as_list = best_features.values.tolist()\n",
    "print('Chi squared says our best features are {}.'.format(bf_as_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) and [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) ([also](http://scikit-learn.org/stable/modules/lda_qda.html#lda-qda)) are often used to find useful combinations of dimensions.\n",
    "\n",
    "* [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
    "* [LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "X = df.iloc[:,[0,1,2,3]]\n",
    "y = df.iloc[:, [4]].values.ravel()\n",
    "\n",
    "# Run PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# Describe\n",
    "print('Our explained variance ratios are {}.'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction\n",
    "\n",
    "A lot of times you will want to turn large bodies of [text into numbers](https://en.wikipedia.org/wiki/Text_mining). You can do this with sparse matrices and [tf-idf transformation](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). This will allow you to feed text data to your ML algorithms in a sane manner. See also the Natural Language Toolkit ([NLTK](https://en.wikipedia.org/wiki/Natural_Language_Toolkit))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CFPB data\n",
    "df = pd.read_csv('data/cfpb.csv', nrows=5000, encoding='cp1252')\n",
    "\n",
    "# Clean up. Do as I say not as I do. Always train test split.\n",
    "dataset = df.dropna(subset=['Consumer complaint narrative'])\n",
    "series = dataset['Consumer complaint narrative']\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Create model.\n",
    "svc = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Vectorize.\n",
    "X = vectorizer.fit_transform(series.values)\n",
    "y = dataset['Product']\n",
    "\n",
    "# Fit model.\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Dummy complaints\n",
    "complaints = [\n",
    "    'The payday loan store charges too much interest and rolled me into a new loan.',\n",
    "    'Debt collectors are calling me 24 hours a day. I do not have the money.',\n",
    "    'oh no how did i get here i am not good with computer'\n",
    "]\n",
    "\n",
    "# Transform\n",
    "complaints_processed = vectorizer.transform(complaints)\n",
    "\n",
    "# Predict classes.\n",
    "print('Predictions:')\n",
    "print(svc.predict(complaints_processed))\n",
    "\n",
    "# Predict proba\n",
    "probability = svc.predict_proba(complaints_processed)\n",
    "categories  = svc.classes_\n",
    "pd.DataFrame(\n",
    "    data=probability,\n",
    "    columns=categories,\n",
    "    index=complaints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Learing Resources\n",
    "\n",
    "* ### [Sklearn Preprocessing Data](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "* ### [Sklearn Feature Selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)\n",
    "* ### [Sklearn Text Analysis](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "* ### [Sklearn Model Selection and Evaluation](http://scikit-learn.org/stable/model_selection.html)\n",
    "* ### [Sklearn Dataset Transformations](http://scikit-learn.org/stable/data_transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Up: [Modeling](4_modeling.ipynb)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"margin-left: 0;\" src=\"static/svm_hyperplanes.svg\" width=\"20%\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='left'>\n",
    "    Image courtesy of <a href='https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg'>ZackWeinberg</a> under the <a href='https://creativecommons.org/licenses/by-sa/3.0/deed.en'>CC BY-SA 3.0</a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
