{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://github.com/theonaunheim\">\n",
    "    <img style=\"border-radius: 100%; float: right;\" src=\"static/strawberry_thief_square.png\" width=10% alt=\"Theo Naunheim's Github\">\n",
    "</a>\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"display: table; width: 100%\">\n",
    "    <div style=\"display: table-row; width: 100%;\">\n",
    "        <div style=\"display: table-cell; width: 50%; vertical-align: middle;\">\n",
    "                <img style=\"display: inline; overflow: hidden; width: 70%;\" src=\"static/sklearn_logo.png\">\n",
    "            <br>\n",
    "            <br>\n",
    "            <ul style=\"display: inline-block\">\n",
    "                <li>\n",
    "                    <a href=\"http://scikit-learn.org/stable/\">Scikit-Learn Home</a>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"http://scikit-learn.org/stable/documentation.html\">Scikit Documentation and User Guides</a>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"http://scikit-learn.org/stable/modules/classes.html\">Scikit API Reference</a>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <a href=\"https://en.wikipedia.org/wiki/Scikit-learn\">Scikit Wikipedia</a>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 10%\">\n",
    "        </div>\n",
    "        <div style=\"display: table-cell; width: 40%; vertical-align: middle;\">\n",
    "            <blockquote>\n",
    "                <p style=\"font-style: italic;\">Essentially, all models are wrong, but some are useful.</p>\n",
    "                <br>\n",
    "                <p>-George E. P. Box</p>\n",
    "            </blockquote>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally\n",
    "\n",
    "Scikit-Learn is a Python library that provides a consistent and relatively painless API for a number of common [machine learning](http://scikit-learn.org/stable/modules/classes.html) algorithms. For our purposes, you can think of machine learning as a mechanism to create mathematical models that learn from data.\n",
    "\n",
    "The Scikit-Learn package provides interfaces for [supervised](https://en.wikipedia.org/wiki/Supervised_learning) and [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) techniques like:\n",
    "* [Naive Bayes Classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "* [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "* [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "* [K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "* [K-Nearest Neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "* [Unoptimized Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "And supporting functions for things like:\n",
    "* Ensemble methods ([Random Forests](https://en.wikipedia.org/wiki/Random_forest))\n",
    "* Model validation / selection ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) / [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) / [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))\n",
    "* Text analysis preprocessing tools [(TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))\n",
    "* Dimensionality reduction techniques ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis))\n",
    "* Hyperparameter tuning ([Exhaustive grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Caveats\n",
    "\n",
    "* I am not an expert on machine learning (or on anything else for that matter), so you should view all my claims with suspicion.\n",
    "\n",
    "* We will not be going into the principles behind these algorithms. If you want to do learn about machine learning from a mathematical perspective, take a look at [Fundementals of Machine Learning for Predictive Data Analytics](https://www.amazon.com/Fundamentals-Machine-Learning-Predictive-Analytics/dp/0262029448).\n",
    "\n",
    "* If you plan to use machine learning (or models in any capacity) for your employer, you may be subject to model risk management rules.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we use sklearn for?\n",
    "\n",
    "In general, we will use sklearn when we have data and we want to use that data to conduct a classification (where we are determining a category or *discrete variable*) or a regression (where we are looking for a number on a continuous distribution or *continuous* variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will we use sklearn for supervised learning?\n",
    "\n",
    "![raw_chart](static/supervised_ml_flowchart_raw.png)\n",
    "\n",
    "This looks complicated, but it's not hard once you get used to it.\n",
    "\n",
    "There are two fundemental steps: the creation of a predictive model and the usage of this model to make a prediction.\n",
    "\n",
    "### Model creation\n",
    "\n",
    "Ignore everything on the green pathway for now. What we are doing conceptually is taking a set of features (charactaristics of each sample) and matching them up with labels (target item that we will use our model to determine).\n",
    "\n",
    "So say we want to make classifier that tells Great White Sharks ('SHARK') from Cod fish ('COD'). We go out, we catch some cod and some sharks, we weigh and measure their length.\n",
    "\n",
    "| Weight (kilos)  | Length (meters)   | Target  |\n",
    "|---|---|---|\n",
    "|700|5|SHARK|\n",
    "|100|2|COD|\n",
    "|650|4|SHARK|\n",
    "|90|2|COD|\n",
    "|600|4|SHARK|\n",
    "\n",
    "In other words, we take data and translate it into 5 separate [weight, length] feature vectors [[700, 5], [100, 2], [650, 4], [90, 2], [600, 4]]. This is the same length as our target vector with labels: [SHARK, COD, SHARK, COD, SHARK].\n",
    "\n",
    "For the purposes of this section we will use X to refer to a matrix, and y to refer to the label/target vector.\n",
    "\n",
    "We plug in our feature vectors (X) and our labels into our machine learning algorithm (along with any hyperparameters, which are non-data elements that speak to how the machine learning algorithm itself will operate). Our output is a predictive model.\n",
    "\n",
    "The data we use to train the algorithm is called the *training data set*. It is separate from our *testing data set*.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "With this model, we can then take a feature vector and predict what class/target/label it will belong to. So for example, if we had\n",
    "\n",
    "| Weight (kilos)  | Length (meters)   |\n",
    "|---|---\n",
    "|600|3|\n",
    "|80|1|\n",
    "|650|5|\n",
    "\n",
    "Intuitively, we can see we should get out [SHARK, COD, SHARK].\n",
    "\n",
    "The data we use to make predictions is the *test data set*. We separate training and testing data to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How do we interact with Sklearn?\n",
    "\n",
    "![annotated_chart](static/supervised_ml_flowchart_annotated.png)\n",
    "\n",
    "\n",
    "### Model Creation\n",
    "\n",
    "First, we take our inputs, and if necessary we `fit_transform()` them into our feature vectors that the machine learning algorithm can digest. Certain types of data, such as text, must be transformed into numbers. Other types of algorithms work better when [normalized](https://en.wikipedia.org/wiki/Feature_scaling). Generally, models will take numpy array objects filled with numbers.\n",
    "\n",
    "Second, we `fit()` our data (X) and labels (y) with our algorithm, which gives us our model.\n",
    "\n",
    "### Model Prediction\n",
    "\n",
    "First we take our inputs for the items we want to predict. We `fit_transform()` them into vectors if necessary, and then we use the `predict()` to determine the predicted classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we will use a simple logistic regression.\n",
    "# https://en.wikipedia.org/wiki/Logistic_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "###\n",
    "# First we create the model\n",
    "###\n",
    "\n",
    "# First we instantiate/create the model we want to use\n",
    "# You supply usually supply model parameters at creation\n",
    "log_reg = LogisticRegression(random_state=True)\n",
    "\n",
    "# Then we add a scalar (optional)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Input data manually (never do this) as 5 samples of 2 features \n",
    "train_data = np.array([[700.0, 5.0],\n",
    "                       [100.0, 2.0],\n",
    "                       [650.0, 5.0],\n",
    "                       [ 80.0, 2.0],\n",
    "                       [600.0, 3.0]])\n",
    "\n",
    "# Normalize data for analysis.\n",
    "train_vectors = scaler.fit_transform(train_data)\n",
    "print('Normalized data:\\n')\n",
    "print(train_vectors)\n",
    "\n",
    "# Set up our targets manually (never do this)\n",
    "targets = np.array(['SHARK', 'COD', 'SHARK', 'COD', 'SHARK'])\n",
    "\n",
    "# Then we fit our model to the data\n",
    "log_reg.fit(train_vectors, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Second we make our predictions\n",
    "###\n",
    "\n",
    "# Put above data into vectors manually\n",
    "test_data = np.array([[600.0, 3.6], \n",
    "                      [ 80.0, 1.5], \n",
    "                      [650.0, 4.5]])\n",
    "\n",
    "# Fit transform so our inputs are on the same scale\n",
    "test_vectors = scaler.fit_transform(test_data)\n",
    "\n",
    "# Run a basic prediction of our three entries\n",
    "prediction = log_reg.predict(test_vectors)\n",
    "\n",
    "# Get proababilities of our three entries\n",
    "probabilities = log_reg.predict_proba(test_vectors)\n",
    "\n",
    "# Get classes\n",
    "classes = log_reg.classes_\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = log_reg.coef_\n",
    "\n",
    "print(prediction, '\\n')\n",
    "print(log_reg.coef_, '\\n')\n",
    "\n",
    "pd.DataFrame(data=probabilities, columns=log_reg.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary to help visualize.\n",
    "x = np.arange(-3, 4, .1)\n",
    "y = np.arange(-3, 4, .1)\n",
    "# Create meshgrid\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate for each value on the grid\n",
    "Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Turn cod and shark into numeric features\n",
    "np.place(Z, Z == 'COD', '001')\n",
    "np.place(Z, Z == 'SHARK', '00000')\n",
    "Z = Z.astype(int)\n",
    "\n",
    "# Graph mesh and scatter.\n",
    "plt.figure(1, figsize=(6, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(train_vectors.T[0], train_vectors.T[1])\n",
    "\n",
    "# Add text for clarity\n",
    "plt.annotate('Sharks, yo.', (0, 2), (1,2), arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.annotate('DAT COD, THO.', (-2, -2))\n",
    "plt.gca().set_xlabel('Weight Scaled Score')\n",
    "plt.gca().set_ylabel('Length Scaled Score')\n",
    "plt.gca().set_xlim(-3, 3)\n",
    "plt.gca().set_ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "You will interact with most sklearn estimators in a stanrdard way. You will `fit()` or `fit_transform()`, and then you'll `predict()`. Because these estimators perform in a similar way, we can put these estimators into \"pipelines\" that can be treated as functional units.\n",
    "\n",
    "To show you the benefits of pipelines, we will be creating a text classifier from publically availible CFPB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create df\n",
    "df = pd.read_csv('data/cfpb.csv')\n",
    "\n",
    "# Lets use product as the identifier\n",
    "df = df[['Product', 'Consumer complaint narrative']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Create Pipeline\n",
    "###\n",
    "\n",
    "# First we import the Pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import vectorizer\n",
    "# This translate words into vectors (one dimension per word)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import TFIDF scaler: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "# This helps adjust for word frequency and importance\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Import Stochastic Gradient Decent classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Now make our pipeline\n",
    "# Supply our pipeline name and process\n",
    "pipeline = Pipeline([('vec', CountVectorizer()),\n",
    "                     ('trans', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='log', max_iter=5))])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Fit our text to our pipeline\n",
    "###\n",
    "\n",
    "# Complaint vector (notice pandas to numpy translation)\n",
    "text_vec = df['Consumer complaint narrative']\n",
    "\n",
    "# Type of product vector\n",
    "target_vec = df['Product']\n",
    "\n",
    "# These pipeline meta esimators only require fit() and predict()\n",
    "pipeline.fit(text_vec, target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_complaints = ['Bank of America are such jerks. They screwed up my mortgage payments and then put me out of my home. I tried to get a loan modification program under HAMP but they screwed that up to.',\n",
    "                    'JPMorgan Chase is the devil. My credit card was fraudulently used and my line of credit was screwed up and my credit score.',\n",
    "                    'Wells Fargo\\'s logo is a stupid color. Red and yellow is an ugly combo for a bank.']\n",
    "                    \n",
    "pipeline.predict(dummy_complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Sklearn also has tools for helping us validate how good our models are.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import convenience method for spliting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import basic accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Randomly divide into train and test\n",
    "train, test = train_test_split(df, test_size=.2)\n",
    "\n",
    "train_X = train['Consumer complaint narrative']\n",
    "train_y = train['Product']\n",
    "test_X = test['Consumer complaint narrative']\n",
    "test_y = test['Product']\n",
    "\n",
    "print('{} training samples.'.format(len(train_X)))\n",
    "print('{} testing samples.'.format(len(test_X)))\n",
    "\n",
    "# We need to re-fit our pipeline to our training data\n",
    "pipeline.fit(train_X, train_y)\n",
    "\n",
    "# Conduct a simple accuracy score. First predict\n",
    "predicted_test_y = pipeline.predict(test_X)\n",
    "true_test_y = test_y\n",
    "\n",
    "# Types of scores here: http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "score = accuracy_score(true_test_y, predicted_test_y)\n",
    "print('Our accuracy score was {:.1%}.'.format(score))\n",
    "\n",
    "# This is essentially calculating the percent of correct predictions.\n",
    "pd.DataFrame({'Predicted': predicted_test_y,\n",
    "              'True': true_test_y}).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the built in cross validation tools\n",
    "# https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "# Import cross validation score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "# Will be sklearn.model_selection in future versions\n",
    "\n",
    "# Because this will create and test our model 3 times, a data subset to ease comutation\n",
    "subset = df.sample(frac=.5)\n",
    "\n",
    "# This takes our model and data, outputs series of scores\n",
    "cvs = cross_val_score(# model\n",
    "                      pipeline,\n",
    "                      # Features\n",
    "                      subset['Consumer complaint narrative'],\n",
    "                      # Target\n",
    "                      subset['Product'],\n",
    "                      # Number of folds\n",
    "                      cv=3,\n",
    "                      # Scoring methodology\n",
    "                      scoring='f1')\n",
    "\n",
    "print(cvs)\n",
    "print(cvs.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter selection\n",
    "\n",
    "We can do an exhaustive grid search of hyperparameters, and get the best model. The built in sklearn estimators all have loss functions baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a long time. Skipping.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Remember what our pipeline looks like:\n",
    "pipeline = Pipeline([('vec', CountVectorizer()),\n",
    "                     ('trans', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='log', max_iter=5))])\n",
    "\n",
    "# Put all the parameters we want to search through in a dict\n",
    "parameters = {'vec__ngram_range': [(1, 1)],\n",
    "              'trans__use_idf': (True, False),\n",
    "              'clf__loss': ['hinge']}\n",
    "\n",
    "# Setup a meta-classifier\n",
    "optimized_clf = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "\n",
    "# Fit metaclassifier\n",
    "optimized_clf.fit(df['Consumer complaint narrative'],\n",
    "                  df['Product'])\n",
    "\n",
    "# Use best model as a predictor\n",
    "print(optimized_clf.predict(['I hate Citibank because they screwed up my mortgage']))\n",
    "\n",
    "# Get best parameters\n",
    "print(optimized_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Which models should I use?\n",
    "\n",
    "There are entire books devoted to this topic. The below chart was created as a joke by the sklearn community, but it's not actually half bad. When in doubt, test all your models and choose the one that performs best.\n",
    "\n",
    "![Scikit Model Chart](static/ml_map.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Links\n",
    "* ### [Scikit-Learn Getting Started](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/text_analytics/general_concepts.html)\n",
    "* ### [Scikit-Learn Tutorials](http://scikit-learn.org/stable/tutorial/)\n",
    "* ### [PyCon 2015: Getting Started With Scikit-Learn (Part 1)](https://www.youtube.com/watch?v=HC0J_SPm9co)\n",
    "* ### [PyCon 2015: Getting Started with Scikit-Learn (Part 2)](https://www.youtube.com/watch?v=oGqGxvqA9-k)\n",
    "* ### [Scikit-Learn Text Tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Up: [Other Useful Stuff](9_other.ipynb)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
